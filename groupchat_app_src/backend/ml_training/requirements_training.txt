# Fine-tuning Requirements
# Install these packages for training the LLM

# Core training libraries
unsloth>=2024.1  # Optimized LLM training
transformers>=4.36.0
datasets>=2.14.0
peft>=0.7.0  # Parameter-Efficient Fine-Tuning (LoRA)
trl>=0.7.0  # Transformer Reinforcement Learning (SFTTrainer)

# PyTorch (install the version matching your CUDA)
torch>=2.1.0
accelerate>=0.25.0

# Utilities
bitsandbytes>=0.41.0  # 4-bit quantization
scipy>=1.11.0
sentencepiece>=0.1.99
protobuf>=3.20.0

# Optional: for better performance
triton>=2.1.0
xformers>=0.0.23  # Memory-efficient attention

# For inference serving (llama.cpp compatible)
llama-cpp-python>=0.2.0
